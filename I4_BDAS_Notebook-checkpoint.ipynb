{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarlanAlternative/I4/blob/main/I4_BDAS_Notebook-checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxXqkelwbzpJ"
      },
      "source": [
        "# Iteration 4: BDAS - PySpark + Colab + AWS\n",
        "## Life Expectancy Prediction with Spark MLlib\n",
        "\n",
        "This notebook implements the complete data science pipeline using PySpark for big data analytics based on the I3.py analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8rmQXSnbzpK"
      },
      "source": [
        "## 1. Environment Setup and Spark Installation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SdIbDex9bzpK",
        "outputId": "61286e41-02c2-4b0f-c2f5-085ae3d23488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,287 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,812 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,374 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,778 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,988 kB]\n",
            "Fetched 25.3 MB in 3s (7,285 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Collecting pyspark==3.5.0\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==3.5.0) (0.10.9.7)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425346 sha256=156183c61656210f224435ae0538e71f1dddc306682277be98cfdb5974cfed88\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/40/20/65eefe766118e0a8f8e385cc3ed6e9eb7241c7e51cfc04c51a\n",
            "Successfully built pyspark\n",
            "Installing collected packages: findspark, pyspark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.1\n",
            "    Uninstalling pyspark-3.5.1:\n",
            "      Successfully uninstalled pyspark-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.3 requires pyspark[connect]~=3.5.1, but you have pyspark 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed findspark-2.0.1 pyspark-3.5.0\n",
            "Spark version: 3.5.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7960aff547d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5e371bfe9912:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>LifeExpectancyPrediction</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Install required packages for Spark environment\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download and setup Spark 3.5.0\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# Install Python packages\n",
        "!pip install findspark pyspark==3.5.0\n",
        "\n",
        "import os\n",
        "import findspark\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import *\n",
        "from pyspark.ml.regression import *\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"LifeExpectancyPrediction\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "spark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR3CWxAGbzpL"
      },
      "source": [
        "## 2. 导入并检查数据\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gZoPZr4TbzpL",
        "outputId": "ea0f2685-7854-4997-e7be-aa49e2c42c68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/Life Expectancy Data.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2655503883.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 从 CSV 加载 WHO Life Expectancy 数据集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Life Expectancy Data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 自动推断字段类型、打印 schema 与前 5 行样例\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data Schema:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/Life Expectancy Data.csv."
          ]
        }
      ],
      "source": [
        "# 从 CSV 加载 WHO Life Expectancy 数据集\n",
        "df = spark.read.options(header=True, inferSchema=True).csv(\"Life Expectancy Data.csv\")\n",
        "\n",
        "# 自动推断字段类型、打印 schema 与前 5 行样例\n",
        "print(\"Data Schema:\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "df.show(5)\n",
        "\n",
        "# 统计总行数与各列缺失数量\n",
        "total_rows = df.count()\n",
        "print(f\"\\nTotal rows: {total_rows}\")\n",
        "\n",
        "print(\"\\nMissing values per column:\")\n",
        "missing_stats = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
        "missing_stats.show(1, vertical=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9w_HfMWbzpL"
      },
      "source": [
        "## 3. 清洗数据\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSUjojpibzpL"
      },
      "outputs": [],
      "source": [
        "# 转换数据类型和处理缺失值\n",
        "numeric_columns = ['Life expectancy ', 'Adult Mortality', 'infant deaths', 'Alcohol',\n",
        "                   'percentage expenditure', 'Hepatitis B', 'Measles ', ' BMI ',\n",
        "                   'under-five deaths ', 'Polio', 'Total expenditure', 'Diphtheria ',\n",
        "                   ' HIV/AIDS', 'GDP', 'Population', ' thinness  1-19 years',\n",
        "                   ' thinness 5-9 years', 'Income composition of resources', 'Schooling']\n",
        "\n",
        "# 转换为适当的数据类型\n",
        "df_clean = df\n",
        "for col_name in numeric_columns:\n",
        "    if col_name in df.columns:\n",
        "        df_clean = df_clean.withColumn(col_name, col(col_name).cast(\"double\"))\n",
        "\n",
        "# 确保 Year 是整数\n",
        "df_clean = df_clean.withColumn(\"Year\", col(\"Year\").cast(\"int\"))\n",
        "\n",
        "# 用均值填补数值缺失\n",
        "numeric_means = {}\n",
        "for col_name in numeric_columns:\n",
        "    if col_name in df_clean.columns:\n",
        "        mean_val = df_clean.select(mean(col(col_name))).collect()[0][0]\n",
        "        numeric_means[col_name] = mean_val if mean_val is not None else 0.0\n",
        "\n",
        "for col_name, mean_val in numeric_means.items():\n",
        "    df_clean = df_clean.withColumn(col_name, when(col(col_name).isNull(), mean_val).otherwise(col(col_name)))\n",
        "\n",
        "# 用 \"Developing\" 填补 Status\n",
        "df_clean = df_clean.withColumn(\"Status\", when(col(\"Status\").isNull(), \"Developing\").otherwise(col(\"Status\")))\n",
        "\n",
        "# 校正不合理值（如疫苗覆盖率 > 100 或 < 0）\n",
        "vaccine_cols = ['Hepatitis B', 'Polio', 'Diphtheria ']\n",
        "for vaccine_col in vaccine_cols:\n",
        "    if vaccine_col in df_clean.columns:\n",
        "        df_clean = df_clean.withColumn(vaccine_col,\n",
        "            when(col(vaccine_col) > 100, 100.0)\n",
        "            .when(col(vaccine_col) < 0, 0.0)\n",
        "            .otherwise(col(vaccine_col)))\n",
        "\n",
        "print(\"Data cleaning completed.\")\n",
        "print(\"\\nSummary statistics:\")\n",
        "df_clean.describe().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsKEoOwobzpL"
      },
      "source": [
        "## 4. 构造新特征\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHzCkIzfbzpM"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import log, log1p\n",
        "\n",
        "# 基于 I3.py 特征工程部分创建新特征\n",
        "df_features = df_clean\n",
        "\n",
        "# 1. log_GDP：GDP 取对数\n",
        "df_features = df_features.withColumn(\"log_GDP\", log1p(col(\"GDP\") + 1e-6))\n",
        "\n",
        "# 2. immunization_avg：三种疫苗平均\n",
        "vaccine_cols = ['Hepatitis B', 'Polio', 'Diphtheria ']\n",
        "available_vaccine_cols = [c for c in vaccine_cols if c in df_features.columns]\n",
        "if available_vaccine_cols:\n",
        "    df_features = df_features.withColumn(\"immunization_avg\",\n",
        "        sum(*[col(vc) for vc in available_vaccine_cols]) / len(available_vaccine_cols))\n",
        "\n",
        "# 3. mortality_ratio：婴儿死亡率比\n",
        "if 'infant deaths' in df_features.columns and 'under-five deaths ' in df_features.columns and 'Adult Mortality' in df_features.columns:\n",
        "    df_features = df_features.withColumn(\"child_mortality_rate\",\n",
        "        col('infant deaths') + col('under-five deaths '))\n",
        "    df_features = df_features.withColumn(\"mortality_ratio\",\n",
        "        col('child_mortality_rate') / (col('Adult Mortality') + 1e-6))\n",
        "\n",
        "# 4. health_spend_pc：人均健康支出\n",
        "if 'percentage expenditure' in df_features.columns and 'Population' in df_features.columns:\n",
        "    df_features = df_features.withColumn(\"health_spend_pc\",\n",
        "        col('percentage expenditure') / (col('Population') + 1e-6))\n",
        "\n",
        "# 基于 I3.py 的额外特征\n",
        "if 'percentage expenditure' in df_features.columns and 'GDP' in df_features.columns:\n",
        "    df_features = df_features.withColumn('health_expenditure_ratio',\n",
        "        col('percentage expenditure') / (col('GDP') + 1e-6))\n",
        "\n",
        "disease_cols = ['Measles ', 'Polio', 'Diphtheria ']\n",
        "if 'GDP' in df_features.columns and 'Income composition of resources' in df_features.columns:\n",
        "    df_features = df_features.withColumn('economic_dev_index',\n",
        "        col('GDP') * col('Income composition of resources'))\n",
        "\n",
        "print(\"New features created.\")\n",
        "new_feature_cols = [c for c in df_features.columns if c not in df_clean.columns]\n",
        "print(f\"New features: {new_feature_cols}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4QdcMH1bzpM"
      },
      "source": [
        "## 5. 整合与丰富数据\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59-KKzdtbzpM"
      },
      "outputs": [],
      "source": [
        "# 为 Country 增加 Region 字段\n",
        "region_data = {\n",
        "    'Afghanistan': 'South Asia', 'Albania': 'Europe', 'Algeria': 'North Africa',\n",
        "    'Argentina': 'South America', 'Armenia': 'Europe', 'Australia': 'Oceania',\n",
        "    'Austria': 'Europe', 'Azerbaijan': 'Europe', 'Bahamas': 'Caribbean',\n",
        "    'Bahrain': 'Middle East', 'Bangladesh': 'South Asia', 'Barbados': 'Caribbean',\n",
        "    'Belarus': 'Europe', 'Belgium': 'Europe', 'Belize': 'Central America',\n",
        "    'Benin': 'West Africa', 'Bhutan': 'South Asia', 'Bolivia': 'South America',\n",
        "    'Bosnia and Herzegovina': 'Europe', 'Botswana': 'Southern Africa', 'Brazil': 'South America',\n",
        "    'Brunei': 'Southeast Asia', 'Bulgaria': 'Europe', 'Burkina Faso': 'West Africa',\n",
        "    'Burundi': 'East Africa', 'Cabo Verde': 'West Africa', 'Cambodia': 'Southeast Asia',\n",
        "    'Cameroon': 'Central Africa', 'Canada': 'North America', 'Central African Republic': 'Central Africa',\n",
        "    'Chad': 'Central Africa', 'Chile': 'South America', 'China': 'East Asia',\n",
        "    'Colombia': 'South America', 'Comoros': 'East Africa', 'Congo': 'Central Africa',\n",
        "    'Costa Rica': 'Central America', \"Cote d'Ivoire\": 'West Africa', 'Croatia': 'Europe',\n",
        "    'Cuba': 'Caribbean', 'Cyprus': 'Europe', 'Czech Republic': 'Europe',\n",
        "    'Denmark': 'Europe', 'Djibouti': 'East Africa', 'Dominican Republic': 'Caribbean',\n",
        "    'Ecuador': 'South America', 'Egypt': 'North Africa', 'El Salvador': 'Central America',\n",
        "    'Eritrea': 'East Africa', 'Estonia': 'Europe', 'Ethiopia': 'East Africa',\n",
        "    'Fiji': 'Oceania', 'Finland': 'Europe', 'France': 'Europe',\n",
        "    'Gabon': 'Central Africa', 'Gambia': 'West Africa', 'Georgia': 'Europe',\n",
        "    'Germany': 'Europe', 'Ghana': 'West Africa', 'Greece': 'Europe',\n",
        "    'Grenada': 'Caribbean', 'Guatemala': 'Central America', 'Guinea': 'West Africa',\n",
        "    'Guinea-Bissau': 'West Africa', 'Guyana': 'South America', 'Haiti': 'Caribbean',\n",
        "    'Honduras': 'Central America', 'Hungary': 'Europe', 'Iceland': 'Europe',\n",
        "    'India': 'South Asia', 'Indonesia': 'Southeast Asia', 'Iran': 'Middle East',\n",
        "    'Iraq': 'Middle East', 'Ireland': 'Europe', 'Israel': 'Middle East',\n",
        "    'Italy': 'Europe', 'Jamaica': 'Caribbean', 'Japan': 'East Asia',\n",
        "    'Jordan': 'Middle East', 'Kazakhstan': 'Central Asia', 'Kenya': 'East Africa',\n",
        "    'Kiribati': 'Oceania', 'Kuwait': 'Middle East', 'Kyrgyzstan': 'Central Asia',\n",
        "    'Laos': 'Southeast Asia', 'Latvia': 'Europe', 'Lebanon': 'Middle East',\n",
        "    'Lesotho': 'Southern Africa', 'Liberia': 'West Africa', 'Libya': 'North Africa',\n",
        "    'Lithuania': 'Europe', 'Luxembourg': 'Europe', 'Madagascar': 'East Africa',\n",
        "    'Malawi': 'East Africa', 'Malaysia': 'Southeast Asia', 'Maldives': 'South Asia',\n",
        "    'Mali': 'West Africa', 'Malta': 'Europe', 'Mauritania': 'North Africa',\n",
        "    'Mauritius': 'East Africa', 'Mexico': 'North America', 'Mongolia': 'East Asia',\n",
        "    'Montenegro': 'Europe', 'Morocco': 'North Africa', 'Mozambique': 'East Africa',\n",
        "    'Myanmar': 'Southeast Asia', 'Namibia': 'Southern Africa', 'Nepal': 'South Asia',\n",
        "    'Netherlands': 'Europe', 'New Zealand': 'Oceania', 'Nicaragua': 'Central America',\n",
        "    'Niger': 'West Africa', 'Nigeria': 'West Africa', 'Norway': 'Europe',\n",
        "    'Oman': 'Middle East', 'Pakistan': 'South Asia', 'Panama': 'Central America',\n",
        "    'Papua New Guinea': 'Oceania', 'Paraguay': 'South America', 'Peru': 'South America',\n",
        "    'Philippines': 'Southeast Asia', 'Poland': 'Europe', 'Portugal': 'Europe',\n",
        "    'Qatar': 'Middle East', 'Romania': 'Europe', 'Russia': 'Europe',\n",
        "    'Rwanda': 'East Africa', 'Samoa': 'Oceania', 'Saudi Arabia': 'Middle East',\n",
        "    'Senegal': 'West Africa', 'Serbia': 'Europe', 'Seychelles': 'East Africa',\n",
        "    'Sierra Leone': 'West Africa', 'Singapore': 'Southeast Asia', 'Slovakia': 'Europe',\n",
        "    'Slovenia': 'Europe', 'Solomon Islands': 'Oceania', 'Somalia': 'East Africa',\n",
        "    'South Africa': 'Southern Africa', 'South Korea': 'East Asia', 'South Sudan': 'East Africa',\n",
        "    'Spain': 'Europe', 'Sri Lanka': 'South Asia', 'Sudan': 'North Africa',\n",
        "    'Suriname': 'South America', 'Swaziland': 'Southern Africa', 'Sweden': 'Europe',\n",
        "    'Switzerland': 'Europe', 'Syria': 'Middle East', 'Tajikistan': 'Central Asia',\n",
        "    'Tanzania': 'East Africa', 'Thailand': 'Southeast Asia', 'Timor-Leste': 'Southeast Asia',\n",
        "    'Togo': 'West Africa', 'Tonga': 'Oceania', 'Trinidad and Tobago': 'Caribbean',\n",
        "    'Tunisia': 'North Africa', 'Turkey': 'Europe', 'Turkmenistan': 'Central Asia',\n",
        "    'Uganda': 'East Africa', 'Ukraine': 'Europe', 'United Arab Emirates': 'Middle East',\n",
        "    'United Kingdom': 'Europe', 'United States of America': 'North America', 'Uruguay': 'South America',\n",
        "    'Uzbekistan': 'Central Asia', 'Vanuatu': 'Oceania', 'Venezuela': 'South America',\n",
        "    'Vietnam': 'Southeast Asia', 'Yemen': 'Middle East', 'Zambia': 'East Africa', 'Zimbabwe': 'East Africa'\n",
        "}\n",
        "\n",
        "region_df = spark.createDataFrame(\n",
        "    [(country, region) for country, region in region_data.items()],\n",
        "    [\"Country\", \"Region\"]\n",
        ")\n",
        "\n",
        "df_enriched = df_features.join(region_df, \"Country\", \"left\")\n",
        "df_enriched = df_enriched.withColumn(\"Region\",\n",
        "    when(col(\"Region\").isNull(), \"Other\").otherwise(col(\"Region\")))\n",
        "\n",
        "print(\"Region distribution:\")\n",
        "df_enriched.groupBy(\"Region\").count().orderBy(desc(\"count\")).show()\n",
        "\n",
        "print(\"\\nStatus distribution:\")\n",
        "df_enriched.groupBy(\"Status\").count().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8kS52mybzpM"
      },
      "source": [
        "## 6. 特征标准化与编码\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0Mp7cbCbzpN"
      },
      "outputs": [],
      "source": [
        "# 准备 ML 管道的特征列\n",
        "exclude_cols = ['Country', 'Year', 'Status', 'Region', 'Life expectancy ']\n",
        "feature_cols = [c for c in df_enriched.columns if c not in exclude_cols]\n",
        "\n",
        "print(f\"Selected feature columns ({len(feature_cols)})\")\n",
        "\n",
        "# 处理特征列中剩余的空值\n",
        "for col_name in feature_cols:\n",
        "    df_enriched = df_enriched.withColumn(col_name,\n",
        "        when(col(col_name).isNull(), 0.0).otherwise(col(col_name)))\n",
        "\n",
        "# 创建索引器和编码器\n",
        "status_indexer = StringIndexer(inputCol=\"Status\", outputCol=\"StatusIndex\")\n",
        "status_encoder = OneHotEncoder(inputCol=\"StatusIndex\", outputCol=\"StatusVec\")\n",
        "region_indexer = StringIndexer(inputCol=\"Region\", outputCol=\"RegionIndex\")\n",
        "region_encoder = OneHotEncoder(inputCol=\"RegionIndex\", outputCol=\"RegionVec\")\n",
        "\n",
        "# 组装所有特征\n",
        "all_feature_cols = feature_cols + [\"StatusVec\", \"RegionVec\"]\n",
        "assembler = VectorAssembler(inputCols=all_feature_cols, outputCol=\"features_raw\")\n",
        "\n",
        "# 标准化特征\n",
        "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\",\n",
        "                       withStd=True, withMean=True)\n",
        "\n",
        "print(\"Feature preprocessing pipeline components defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNTXSr-cbzpN"
      },
      "source": [
        "## 7. 划分训练与测试集\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jS2ij8xbzpN"
      },
      "outputs": [],
      "source": [
        "# 准备带有目标变量的数据集\n",
        "df_ml = df_enriched.filter(col(\"Life expectancy \").isNotNull())\n",
        "\n",
        "# 按 0.8 / 0.2 随机划分（固定 seed）\n",
        "train_df, test_df = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(f\"Training set size: {train_df.count()}\")\n",
        "print(f\"Test set size: {test_df.count()}\")\n",
        "\n",
        "# 缓存结果提高后续效率\n",
        "train_df.cache()\n",
        "test_df.cache()\n",
        "\n",
        "print(\"\\nTrain set sample:\")\n",
        "train_df.select(\"Country\", \"Year\", \"Life expectancy \", \"Status\", \"Region\").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMC-KpM9bzpN"
      },
      "source": [
        "## 8. 建模与比较\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K5jxvUXbzpN"
      },
      "outputs": [],
      "source": [
        "# 创建 ML 管道\n",
        "def create_preprocessing_pipeline():\n",
        "    return Pipeline(stages=[\n",
        "        status_indexer,\n",
        "        region_indexer,\n",
        "        status_encoder,\n",
        "        region_encoder,\n",
        "        assembler,\n",
        "        scaler\n",
        "    ])\n",
        "\n",
        "# 定义要比较的模型\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(featuresCol='features', labelCol='Life expectancy '),\n",
        "    'Decision Tree': DecisionTreeRegressor(featuresCol='features', labelCol='Life expectancy ', seed=42),\n",
        "    'Random Forest': RandomForestRegressor(featuresCol='features', labelCol='Life expectancy ', seed=42),\n",
        "    'GBT': GBTRegressor(featuresCol='features', labelCol='Life expectancy ', seed=42)\n",
        "}\n",
        "\n",
        "# 设置评估指标\n",
        "evaluator_rmse = RegressionEvaluator(labelCol='Life expectancy ', predictionCol='prediction', metricName='rmse')\n",
        "evaluator_mae = RegressionEvaluator(labelCol='Life expectancy ', predictionCol='prediction', metricName='mae')\n",
        "evaluator_r2 = RegressionEvaluator(labelCol='Life expectancy ', predictionCol='prediction', metricName='r2')\n",
        "\n",
        "# 存储结果\n",
        "results = {}\n",
        "best_models = {}\n",
        "\n",
        "# 训练和评估每个模型\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "\n",
        "    full_pipeline = Pipeline(stages=create_preprocessing_pipeline().getStages() + [model])\n",
        "    param_grid = ParamGridBuilder().build()\n",
        "\n",
        "    cv = CrossValidator(\n",
        "        estimator=full_pipeline,\n",
        "        estimatorParamMaps=param_grid,\n",
        "        evaluator=evaluator_r2,\n",
        "        numFolds=5,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    cv_model = cv.fit(train_df)\n",
        "    predictions = cv_model.transform(test_df)\n",
        "\n",
        "    rmse = evaluator_rmse.evaluate(predictions)\n",
        "    mae = evaluator_mae.evaluate(predictions)\n",
        "    r2 = evaluator_r2.evaluate(predictions)\n",
        "\n",
        "    results[model_name] = {'RMSE': rmse, 'MAE': mae, 'R²': r2}\n",
        "    best_models[model_name] = cv_model\n",
        "\n",
        "    print(f\"{model_name} - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "# 显示结果表\n",
        "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
        "print(\"\\nModel Performance Comparison:\")\n",
        "print(results_df.round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYC88Nr1bzpN"
      },
      "outputs": [],
      "source": [
        "# 选择最佳模型\n",
        "best_model_name = max(results.keys(), key=lambda x: results[x]['R²'])\n",
        "best_model = best_models[best_model_name]\n",
        "\n",
        "print(f\"\\nBest model: {best_model_name}\")\n",
        "print(f\"Performance: R² = {results[best_model_name]['R²']:.4f}, RMSE = {results[best_model_name]['RMSE']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EydUaCObzpO"
      },
      "source": [
        "## 9. 运行模型并生成预测结果\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKbkaTaubzpO"
      },
      "outputs": [],
      "source": [
        "# 在 test 集运行预测\n",
        "final_predictions = best_model.transform(test_df)\n",
        "\n",
        "# 输出预测样例\n",
        "print(\"Prediction samples:\")\n",
        "final_predictions.select(\n",
        "    \"Country\", \"Year\",\n",
        "    col(\"Life expectancy \").alias(\"True_Value\"),\n",
        "    col(\"prediction\").alias(\"Predicted_Value\")\n",
        ").show(10)\n",
        "\n",
        "# 保存预测结果\n",
        "final_predictions.select(\n",
        "    \"Country\", \"Year\", \"Status\", \"Region\",\n",
        "    col(\"Life expectancy \").alias(\"True_Value\"),\n",
        "    col(\"prediction\").alias(\"Predicted_Value\")\n",
        ").coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"predictions_output\")\n",
        "\n",
        "print(\"\\nPredictions saved to 'predictions_output' directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73JQdJhVbzpO"
      },
      "source": [
        "## 10. 分析模型输出模式\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Avkfty2EbzpO"
      },
      "outputs": [],
      "source": [
        "# 计算残差统计\n",
        "residuals_df = final_predictions.withColumn(\"residual\",\n",
        "    col(\"Life expectancy \") - col(\"prediction\"))\n",
        "\n",
        "residual_stats = residuals_df.select(\n",
        "    mean(\"residual\").alias(\"mean_residual\"),\n",
        "    stddev(\"residual\").alias(\"std_residual\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"Residual statistics - Mean: {residual_stats['mean_residual']:.4f}, Std: {residual_stats['std_residual']:.4f}\")\n",
        "\n",
        "# 按 Status 分组\n",
        "print(\"\\nAverage predictions by Status:\")\n",
        "residuals_df.groupBy(\"Status\").agg(\n",
        "    mean(\"prediction\").alias(\"avg_prediction\"),\n",
        "    mean(\"Life expectancy \").alias(\"avg_actual\")\n",
        ").show()\n",
        "\n",
        "# 按 Region 分组\n",
        "print(\"\\nAverage predictions by Region:\")\n",
        "residuals_df.groupBy(\"Region\").agg(\n",
        "    mean(\"prediction\").alias(\"avg_prediction\"),\n",
        "    mean(\"Life expectancy \").alias(\"avg_actual\")\n",
        ").orderBy(desc(\"avg_prediction\")).show()\n",
        "\n",
        "# Top 误差样本\n",
        "print(\"\\nTop 10 largest absolute errors:\")\n",
        "top_errors = residuals_df.select(\n",
        "    \"Country\", \"Year\", \"Status\", \"Region\",\n",
        "    \"Life expectancy \", \"prediction\", col(\"residual\").alias(\"error\")\n",
        ").withColumn(\"abs_error\", abs(col(\"error\"))).orderBy(desc(\"abs_error\")).limit(10)\n",
        "top_errors.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fh3KGTmbzpO"
      },
      "outputs": [],
      "source": [
        "# 输出特征重要性\n",
        "if best_model_name in ['Decision Tree', 'Random Forest', 'GBT']:\n",
        "    try:\n",
        "        model_stage = best_model.bestModel.stages[-1]\n",
        "        if hasattr(model_stage, 'featureImportances'):\n",
        "            importances = model_stage.featureImportances\n",
        "            print(f\"\\nFeature importance for {best_model_name}:\")\n",
        "\n",
        "            feature_names = all_feature_cols\n",
        "            importance_list = [(feature_names[i], importances[i]) for i in range(len(feature_names))]\n",
        "            importance_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            for i, (feature, importance) in enumerate(importance_list[:15]):\n",
        "                print(f\"  {i+1:2d}. {feature}: {importance:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Feature importance not available: {e}\")\n",
        "else:\n",
        "    print(\"Feature importance not available for Linear Regression model.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT15NPTgbzpO"
      },
      "source": [
        "## 11. 可视化结果\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAKlFrTRbzpO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 设置 matplotlib 字体支持（基于 I3.py）\n",
        "plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 转换预测结果为 Pandas 用于可视化\n",
        "viz_data = residuals_df.select(\n",
        "    \"Life expectancy \", \"prediction\", \"residual\", \"Status\", \"Region\"\n",
        ").toPandas()\n",
        "\n",
        "# 设置绘图\n",
        "plt.style.use('default')\n",
        "fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "# 1. 绘制 Pred vs True 散点图\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.scatter(viz_data['Life expectancy '], viz_data['prediction'], alpha=0.6, s=20)\n",
        "plt.plot([viz_data['Life expectancy '].min(), viz_data['Life expectancy '].max()],\n",
        "         [viz_data['Life expectancy '].min(), viz_data['Life expectancy '].max()], 'r--', lw=2)\n",
        "plt.xlabel('True Life Expectancy')\n",
        "plt.ylabel('Predicted Life Expectancy')\n",
        "plt.title('Predicted vs True Life Expectancy')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. 绘制 Residual 直方图\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.hist(viz_data['residual'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "plt.axvline(viz_data['residual'].mean(), color='red', linestyle='--', linewidth=2,\n",
        "           label=f'Mean: {viz_data[\"residual\"].mean():.3f}')\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Residual Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Residuals vs Fitted\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.scatter(viz_data['prediction'], viz_data['residual'], alpha=0.6, s=20)\n",
        "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
        "plt.xlabel('Fitted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs Fitted Values')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. 模型比较柱状图\n",
        "plt.subplot(2, 3, 4)\n",
        "model_names = list(results.keys())\n",
        "r2_scores = [results[name]['R²'] for name in model_names]\n",
        "rmse_scores = [results[name]['RMSE'] for name in model_names]\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, r2_scores, width, label='R²', alpha=0.8)\n",
        "plt.bar(x + width/2, np.array(rmse_scores)/max(rmse_scores), width, label='RMSE (normalized)', alpha=0.8)\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xticks(x, [name.replace(' ', '\\n') for name in model_names], rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. 按 Status 的预测\n",
        "plt.subplot(2, 3, 5)\n",
        "status_means = viz_data.groupby('Status').agg({\n",
        "    'Life expectancy ': 'mean',\n",
        "    'prediction': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "x = np.arange(len(status_means))\n",
        "plt.bar(x - 0.2, status_means['Life expectancy '], 0.4, label='Actual', alpha=0.8)\n",
        "plt.bar(x + 0.2, status_means['prediction'], 0.4, label='Predicted', alpha=0.8)\n",
        "plt.xlabel('Status')\n",
        "plt.ylabel('Life Expectancy')\n",
        "plt.title('Predictions by Status')\n",
        "plt.xticks(x, status_means['Status'])\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. 按 Region 的预测（前5个地区）\n",
        "plt.subplot(2, 3, 6)\n",
        "region_means = viz_data.groupby('Region').agg({\n",
        "    'Life expectancy ': 'mean',\n",
        "    'prediction': 'mean'\n",
        "}).reset_index().sort_values('prediction', ascending=False).head(5)\n",
        "\n",
        "x = np.arange(len(region_means))\n",
        "plt.bar(x - 0.2, region_means['Life expectancy '], 0.4, label='Actual', alpha=0.8)\n",
        "plt.bar(x + 0.2, region_means['prediction'], 0.4, label='Predicted', alpha=0.8)\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Life Expectancy')\n",
        "plt.title('Predictions by Region (Top 5)')\n",
        "plt.xticks(x, region_means['Region'], rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('spark_ml_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Visualization saved as 'spark_ml_results.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IhROVkobzpP"
      },
      "source": [
        "## 12. 模型再迭代（Iteration 2 within Iteration 4）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ggcf_xrDbzpP"
      },
      "outputs": [],
      "source": [
        "# 在主模型上扩大 numTrees / maxDepth 网格重新调参\n",
        "print(f\"\\nPerforming enhanced hyperparameter tuning for {best_model_name}...\")\n",
        "\n",
        "# 根据模型类型定义参数网格\n",
        "if best_model_name == 'Random Forest':\n",
        "    param_grid = ParamGridBuilder() \\\n",
        "        .addGrid(models['Random Forest'].numTrees, [50, 100, 200]) \\\n",
        "        .addGrid(models['Random Forest'].maxDepth, [5, 10, 15]) \\\n",
        "        .build()\n",
        "elif best_model_name == 'Decision Tree':\n",
        "    param_grid = ParamGridBuilder() \\\n",
        "        .addGrid(models['Decision Tree'].maxDepth, [5, 10, 15, 20]) \\\n",
        "        .build()\n",
        "elif best_model_name == 'GBT':\n",
        "    param_grid = ParamGridBuilder() \\\n",
        "        .addGrid(models['GBT'].maxIter, [50, 100, 200]) \\\n",
        "        .addGrid(models['GBT'].maxDepth, [3, 5, 7]) \\\n",
        "        .build()\n",
        "else:\n",
        "    param_grid = ParamGridBuilder() \\\n",
        "        .addGrid(models['Linear Regression'].regParam, [0.0, 0.01, 0.1]) \\\n",
        "        .build()\n",
        "\n",
        "# 创建增强交叉验证器\n",
        "enhanced_cv = CrossValidator(\n",
        "    estimator=Pipeline(stages=create_preprocessing_pipeline().getStages() + [models[best_model_name]]),\n",
        "    estimatorParamMaps=param_grid,\n",
        "    evaluator=evaluator_r2,\n",
        "    numFolds=5,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# 拟合增强模型\n",
        "enhanced_model = enhanced_cv.fit(train_df)\n",
        "enhanced_predictions = enhanced_model.transform(test_df)\n",
        "\n",
        "# 计算增强指标\n",
        "enhanced_rmse = evaluator_rmse.evaluate(enhanced_predictions)\n",
        "enhanced_r2 = evaluator_r2.evaluate(enhanced_predictions)\n",
        "\n",
        "# 比较两轮 RMSE / R² 变化\n",
        "print(f\"\\nEnhanced {best_model_name} Results:\")\n",
        "print(f\"Original R²: {results[best_model_name]['R²']:.4f}\")\n",
        "print(f\"Enhanced R²: {enhanced_r2:.4f}\")\n",
        "print(f\"Original RMSE: {results[best_model_name]['RMSE']:.4f}\")\n",
        "print(f\"Enhanced RMSE: {enhanced_rmse:.4f}\")\n",
        "\n",
        "# 保存两轮预测结果对比\n",
        "comparison_results = {\n",
        "    'Model': [f'{best_model_name} (Original)', f'{best_model_name} (Enhanced)'],\n",
        "    'R²': [results[best_model_name]['R²'], enhanced_r2],\n",
        "    'RMSE': [results[best_model_name]['RMSE'], enhanced_rmse]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_results)\n",
        "print(\"\\nModel Enhancement Comparison:\")\n",
        "print(comparison_df.round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT-OR6aWbzpP"
      },
      "outputs": [],
      "source": [
        "# 保存增强预测结果\n",
        "enhanced_predictions.select(\n",
        "    \"Country\", \"Year\", \"Status\", \"Region\",\n",
        "    col(\"Life expectancy \").alias(\"True_Value\"),\n",
        "    col(\"prediction\").alias(\"Enhanced_Prediction\")\n",
        ").coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"enhanced_predictions\")\n",
        "\n",
        "print(\"Enhanced predictions saved to 'enhanced_predictions' directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8HuL05PbzpP"
      },
      "source": [
        "## 13. 收尾准备报告\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce-1PgOBbzpP"
      },
      "outputs": [],
      "source": [
        "# 保存最佳增强模型\n",
        "enhanced_model.write().overwrite().save(\"best_life_expectancy_model\")\n",
        "\n",
        "# 最终总结\n",
        "print(\"=\"*60)\n",
        "print(\"ITERATION 4 - BDAS PYSPARK ML PIPELINE SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nDataset Information:\")\n",
        "print(f\"  - Total records: {df_ml.count():,}\")\n",
        "print(f\"  - Training samples: {train_df.count():,}\")\n",
        "print(f\"  - Test samples: {test_df.count():,}\")\n",
        "print(f\"  - Features after engineering: {len(all_feature_cols)}\")\n",
        "\n",
        "print(f\"\\nModel Performance (Best: {best_model_name}):\")\n",
        "for model_name, metrics in results.items():\n",
        "    marker = \" << BEST\" if model_name == best_model_name else \"\"\n",
        "    print(f\"  - {model_name}: R² = {metrics['R²']:.4f}, RMSE = {metrics['RMSE']:.4f}{marker}\")\n",
        "\n",
        "print(f\"\\nEnhanced Model Results:\")\n",
        "print(f\"  - Enhanced R²: {enhanced_r2:.4f}\")\n",
        "print(f\"  - Enhanced RMSE: {enhanced_rmse:.4f}\")\n",
        "print(f\"  - Improvement in R²: {(enhanced_r2 - results[best_model_name]['R²']):.4f}\")\n",
        "\n",
        "print(f\"\\nOutput Files Generated:\")\n",
        "print(f\"  - predictions_output/ : Original model predictions\")\n",
        "print(f\"  - enhanced_predictions/ : Enhanced model predictions\")\n",
        "print(f\"  - best_life_expectancy_model/ : Saved ML model\")\n",
        "print(f\"  - spark_ml_results.png : Visualization charts\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w-TIWzKbzpP"
      },
      "outputs": [],
      "source": [
        "# 清理和结束环境\n",
        "train_df.unpersist()\n",
        "test_df.unpersist()\n",
        "\n",
        "# spark.stop()\n",
        "\n",
        "print(\"Notebook execution completed. Spark session is ready for further use.\")\n",
        "print(\"To stop the Spark session, uncomment the spark.stop() line above.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}